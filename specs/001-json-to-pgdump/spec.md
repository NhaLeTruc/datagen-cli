# Feature Specification: JSON Schema to PostgreSQL Dump Generator

**Feature Branch**: `001-json-to-pgdump`
**Created**: 2025-11-15
**Status**: Draft
**Input**: User description: "A command-line tool that transforms declarative JSON schema definitions into fully-formed PostgreSQL dump files containing realistic mock data, without requiring a running PostgreSQL instance."

## User Scenarios & Testing *(mandatory)*

### User Story 1 - Basic Schema to Dump Conversion (Priority: P1)

A developer needs to create a test database with realistic data for local development. They define a simple schema with 2-3 tables in JSON format, specifying table names, columns, data types, and basic constraints. They run the tool and receive a valid PostgreSQL dump file that can be restored using standard PostgreSQL tools.

**Why this priority**: This is the minimum viable product. Without the ability to transform a basic schema into a working dump file, the tool provides no value. This establishes the core workflow and validates the fundamental architecture.

**Independent Test**: Can be fully tested by creating a JSON schema with simple tables (e.g., users, posts), running the tool to generate a dump file, and successfully restoring it to a PostgreSQL instance using pg_restore. The restored database should contain the defined schema with realistic data.

**Acceptance Scenarios**:

1. **Given** a JSON schema defining a "users" table with id, name, email, created_at columns, **When** the tool processes this schema, **Then** a valid PostgreSQL dump file is generated that can be restored without errors
2. **Given** a schema with referential integrity constraints (e.g., posts.user_id references users.id), **When** the dump is generated, **Then** foreign key relationships are properly maintained and data insertion order respects dependencies
3. **Given** a schema requesting 100 rows per table, **When** the dump is generated, **Then** exactly 100 rows of realistic data are created for each table
4. **Given** an invalid JSON schema (malformed syntax), **When** the tool attempts to process it, **Then** a clear error message is displayed indicating the syntax issue and location
5. **Given** a valid dump file generated by the tool, **When** it is restored using pg_restore, **Then** all tables, constraints, and data are successfully restored to a PostgreSQL database

---

### User Story 2 - Intelligent Context-Aware Data Generation (Priority: P2)

A QA engineer defines a schema where columns have semantic names like "email", "phone_number", "first_name", "postal_code", and "credit_card". The tool automatically generates realistic, properly formatted data for each column without requiring explicit data generation rules.

**Why this priority**: This differentiates the tool from simple random data generators. Realistic data is essential for meaningful testing, demo scenarios, and catching validation bugs. This builds on the P1 foundation by making generated data actually useful.

**Independent Test**: Can be tested by defining a schema with semantic column names, generating a dump, and verifying that emails are valid format, phone numbers match expected patterns, names look realistic, and data makes contextual sense.

**Acceptance Scenarios**:

1. **Given** a column named "email", **When** data is generated, **Then** all values are valid email addresses (contain @, valid domain format)
2. **Given** a column named "phone_number" or "phone", **When** data is generated, **Then** values match standard phone number patterns (e.g., (555) 123-4567 or +1-555-123-4567)
3. **Given** columns named "first_name" and "last_name", **When** data is generated, **Then** values are realistic human names from diverse cultural backgrounds
4. **Given** a column named "postal_code" or "zip_code", **When** data is generated, **Then** values match postal code formats for the specified or default locale
5. **Given** a timestamp column named "created_at" or "updated_at", **When** data is generated, **Then** values are realistic recent dates in chronological order where appropriate

---

### User Story 3 - Custom Data Patterns and Business Rules (Priority: P3)

A data analyst needs to test analytics queries against realistic data distributions. They specify custom data generation rules in the JSON schema, such as "80% of orders should be 'completed', 15% 'pending', 5% 'cancelled'" or "generate timestamps following a time-series pattern with daily seasonality".

**Why this priority**: Enables testing of complex business logic and analytics. While P1 and P2 provide basic functionality, many real-world testing scenarios require specific data distributions and patterns that reflect production realities.

**Independent Test**: Can be tested by specifying distribution rules in the schema (e.g., status field with percentage weights), generating data, and querying the resulting database to verify the distribution matches specifications within acceptable variance.

**Acceptance Scenarios**:

1. **Given** a schema specifying "status" field with distribution weights (completed: 80%, pending: 15%, cancelled: 5%), **When** 1000 rows are generated, **Then** the distribution matches specifications within Â±3% variance
2. **Given** a schema defining a custom data pattern for account numbers (e.g., "ACC-{year}-{sequence}"), **When** data is generated, **Then** values follow the specified pattern
3. **Given** a time-series configuration for order timestamps (daily pattern, weekday bias), **When** data is generated, **Then** timestamp distribution shows realistic daily and weekly patterns
4. **Given** a schema with conditional logic (e.g., "if order_type is 'premium', price must be > $100"), **When** data is generated, **Then** all generated data respects the business rules

---

### User Story 4 - Deterministic Data Generation with Seeds (Priority: P4)

A CI/CD engineer needs to generate consistent test data across multiple pipeline runs to enable regression testing. They specify a seed value in the configuration, and the tool produces identical output every time with that seed.

**Why this priority**: Essential for automated testing and debugging. While lower priority than core functionality, deterministic generation is crucial for CI/CD integration and reproducible bug investigations.

**Independent Test**: Can be tested by running the tool twice with the same schema and seed value, then comparing the output files byte-for-byte or comparing restored database contents.

**Acceptance Scenarios**:

1. **Given** a schema with seed value "12345", **When** the tool is run twice, **Then** both dump files contain identical data
2. **Given** the same schema with different seed values, **When** the tool is run, **Then** the generated data is different but maintains the same statistical properties
3. **Given** a schema without a specified seed, **When** the tool is run multiple times, **Then** each run produces different random data

---

### User Story 5 - Pre-built Scenario Templates (Priority: P5)

A developer starting a new e-commerce project wants a realistic test database without defining schemas from scratch. They select the "e-commerce" template, optionally customize it, and quickly generate a comprehensive database with products, customers, orders, reviews, and inventory.

**Why this priority**: Dramatically improves user experience and time-to-value for common scenarios. However, it depends on the core engine (P1-P3) being solid first. Templates are valuable but not essential for the tool to be useful.

**Independent Test**: Can be tested by selecting a pre-built template (e.g., "ecommerce"), running the tool, and verifying that a complete, realistic database is generated with properly related tables and sensible data.

**Acceptance Scenarios**:

1. **Given** selection of the "ecommerce" template, **When** the tool generates a dump, **Then** the resulting database includes related tables for products, customers, orders, order_items, reviews, and categories with realistic relationships
2. **Given** a template with customization options (e.g., specify product count, customer count), **When** these are specified, **Then** the generated data reflects the customizations while maintaining referential integrity
3. **Given** selection of the "saas" template, **When** the tool generates a dump, **Then** the database includes tenants, users, subscriptions, usage_metrics, and billing tables with appropriate relationships
4. **Given** selection of multiple compatible templates, **When** the tool generates a dump, **Then** the templates are merged cohesively without conflicts

---

### User Story 6 - Multiple Output Format Support (Priority: P6)

A DevOps engineer needs to import test data into different environments with varying PostgreSQL versions and import tool availability. They specify the desired output format (pg_dump custom format, SQL INSERT statements, or COPY format) and receive appropriately formatted output.

**Why this priority**: Improves flexibility and compatibility. However, supporting one format well (P1) is more important than supporting multiple formats. This is an enhancement that broadens applicability.

**Independent Test**: Can be tested by generating the same schema in different output formats and successfully importing each format using the appropriate PostgreSQL tool.

**Acceptance Scenarios**:

1. **Given** output format specification "pg_dump custom", **When** a dump is generated, **Then** the file can be restored using pg_restore
2. **Given** output format specification "sql", **When** a dump is generated, **Then** the file contains valid SQL INSERT statements that can be executed via psql
3. **Given** output format specification "copy", **When** a dump is generated, **Then** the file contains PostgreSQL COPY commands with properly formatted data
4. **Given** no format specification, **When** a dump is generated, **Then** pg_dump custom format is used as the default

---

### Edge Cases

- What happens when a schema defines circular foreign key dependencies (e.g., table A references B, B references A)?
- How does the tool handle schemas with millions of rows requested (memory constraints)?
- What happens when an invalid PostgreSQL data type is specified in the schema?
- How are NULL values handled for nullable vs non-nullable columns?
- What happens when foreign key references point to tables with zero rows?
- How does the tool handle very large JSON schema files (e.g., 100+ tables)?
- What happens when column name patterns are ambiguous (e.g., "user_email" - is it an email or just a string)?
- How are unique constraints enforced when generating large datasets?
- What happens when the specified seed value is invalid or out of range?
- How does the tool handle schemas with PostgreSQL-specific features like arrays, JSONB, or custom enums?

## Requirements *(mandatory)*

### Functional Requirements

**Schema Processing**

- **FR-001**: System MUST accept JSON schema definitions as input via file path or stdin
- **FR-002**: System MUST validate JSON schema syntax and report clear error messages for malformed input
- **FR-003**: System MUST support all standard PostgreSQL data types (integer, bigint, varchar, text, boolean, timestamp, date, numeric, uuid, etc.)
- **FR-004**: System MUST support PostgreSQL-specific types including arrays, JSON/JSONB, and user-defined enums
- **FR-005**: System MUST allow specification of primary keys, foreign keys, unique constraints, and check constraints in the schema
- **FR-006**: System MUST allow specification of indexes in the schema definition
- **FR-007**: System MUST support table relationships (one-to-many, many-to-many via junction tables)

**Data Generation**

- **FR-008**: System MUST generate realistic fake data based on column names (e.g., "email" generates valid emails, "phone" generates phone numbers)
- **FR-009**: System MUST respect referential integrity by generating foreign key values that reference existing primary keys
- **FR-010**: System MUST generate data in dependency order (referenced tables before referencing tables)
- **FR-011**: System MUST allow users to specify exact row counts per table
- **FR-012**: System MUST support deterministic data generation using seed values
- **FR-013**: System MUST generate unique values for columns with unique constraints
- **FR-014**: System MUST respect null/not-null constraints when generating data
- **FR-015**: System MUST support custom data patterns via configuration (e.g., regex patterns, value ranges, enums)
- **FR-016**: System MUST support configurable data distributions (uniform, normal, weighted, sequential)
- **FR-017**: System MUST support time-series data generation with configurable patterns

**Output Generation**

- **FR-018**: System MUST generate valid PostgreSQL dump files that can be restored without errors
- **FR-019**: System MUST support pg_dump custom format as output
- **FR-020**: System MUST include proper dump file headers, encoding declarations, and metadata
- **FR-021**: System MUST generate output without requiring a running PostgreSQL instance
- **FR-022**: System MUST support outputting schema definitions, data, or both based on configuration
- **FR-023**: System MUST support selective table generation (generate data for specified tables only)

**Templates and Presets**

- **FR-024**: System MUST provide pre-built templates for common scenarios (ecommerce, SaaS, healthcare, finance)
- **FR-025**: System MUST allow users to extend and customize templates
- **FR-026**: System MUST support combining multiple templates

**CLI Interface**

- **FR-027**: System MUST provide a command-line interface following standard POSIX conventions
- **FR-028**: System MUST support input via file path argument or stdin
- **FR-029**: System MUST support output to file path or stdout
- **FR-030**: System MUST provide a --help flag with comprehensive usage documentation
- **FR-031**: System MUST provide a --version flag showing tool version
- **FR-032**: System MUST exit with code 0 on success, non-zero on failure
- **FR-033**: System MUST write error messages to stderr
- **FR-034**: System MUST provide progress indicators for long-running operations

**Performance and Scale**

- **FR-035**: System MUST use streaming/incremental data generation to avoid loading entire datasets in memory
- **FR-036**: System MUST support generation of datasets with millions of rows
- **FR-037**: System MUST allow configuration of memory limits and resource constraints

**Validation and Error Handling**

- **FR-038**: System MUST validate that generated dumps are restorable before completing
- **FR-039**: System MUST provide clear error messages for schema validation failures
- **FR-040**: System MUST provide clear error messages for constraint violations during generation
- **FR-041**: System MUST handle invalid column type specifications gracefully

### Assumptions

- Users have basic familiarity with JSON format and relational database concepts
- PostgreSQL tools (pg_restore) are available for users to import generated dumps
- Default locale for data generation is en_US unless otherwise specified
- Default output format is pg_dump custom format unless specified
- Column name pattern matching is case-insensitive (e.g., "Email" and "email" both trigger email generation)
- For ambiguous column names, the tool makes reasonable assumptions (documentable mapping)
- When no seed is specified, system uses a random seed based on current timestamp
- Pre-built templates target PostgreSQL 12+ compatibility
- Maximum reasonable dataset size is 100GB before performance significantly degrades
- Users understand that generated data is synthetic and should not be used for production purposes

### Key Entities

- **Schema Definition**: JSON structure describing tables, columns, types, constraints, and relationships. Attributes include table name, column definitions (name, type, constraints), foreign key relationships, indexes, and data generation rules.

- **Table Specification**: Individual table definition within a schema. Attributes include table name, column list, constraints (primary key, foreign keys, unique, check), row count target, and custom generation rules.

- **Column Specification**: Individual column definition within a table. Attributes include column name, data type, nullable flag, default value, unique constraint, and custom generation pattern.

- **Data Generator**: Logical component responsible for producing realistic data for a specific column type or pattern. Associated with data type, pattern matching rules, and generation strategy.

- **Template**: Pre-built schema definition for common use cases. Attributes include template name, category (ecommerce, SaaS, etc.), table definitions, and customization parameters.

- **Dump File**: Output PostgreSQL dump file. Attributes include format type (custom, SQL, COPY), encoding, PostgreSQL version compatibility, and metadata headers.

## Success Criteria *(mandatory)*

### Measurable Outcomes

- **SC-001**: Users can generate a basic dump file (3 tables, 1000 rows each) in under 5 seconds on standard hardware
- **SC-002**: Generated dump files are successfully restorable to PostgreSQL versions 12, 13, 14, 15, and 16 with 100% success rate
- **SC-003**: 95% of common column name patterns (email, phone, name, address, date, etc.) produce contextually appropriate data without custom configuration
- **SC-004**: Developers can create a realistic test database from a template in under 1 minute (from template selection to restored database)
- **SC-005**: Generated data with seed values produces byte-identical output across 100 consecutive runs
- **SC-006**: Tool can generate datasets of 1 million rows across multiple tables in under 2 minutes
- **SC-007**: 90% of users successfully generate their first dump file without consulting documentation beyond --help
- **SC-008**: Referential integrity is maintained in 100% of generated dumps (no orphaned foreign keys)
- **SC-009**: Tool operates with memory usage under 500MB for datasets up to 100,000 rows
- **SC-010**: Error messages enable users to fix schema issues without external help in 80% of cases